{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Caricamento del dataset (gi√† suddiviso in train e test)\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "\n",
    "# Suddivisione in training e validation set\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.Input(shape=(28, 28)),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(\n",
    "loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=SGD(learning_rate=0.01),\n",
    "metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "X_train, y_train,\n",
    "epochs=30,\n",
    "validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Caricamento dataset\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "#Normalization: adattamento PRIMA del grafo\n",
    "normalization_layer = layers.Normalization()\n",
    "normalization_layer.adapt(X_train)\n",
    "\n",
    "#Layer e struttura manuale\n",
    "hidden1 = layers.Dense(30, activation=\"relu\")\n",
    "hidden2 = layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = layers.Concatenate()\n",
    "outputlayer = layers.Dense(1)\n",
    "\n",
    "#Input simbolico\n",
    "input = layers.Input(shape=X_train.shape[1:])  # es. 8 feature\n",
    "\n",
    "#Costruzione del grafo computazionale\n",
    "normalized = normalization_layer(input)       # normalizzazione (dopo adapt)\n",
    "h1 = hidden1(normalized)                       # primo hidden layer\n",
    "h2 = hidden2(h1)                               # secondo hidden layer\n",
    "concat = concat_layer([normalized, h2])        # percorso wide + deep\n",
    "output = outputlayer(concat)                  # output finale\n",
    "\n",
    "#Costruzione modello\n",
    "model = keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "#Compilazione\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"RootMeanSquaredError\"]\n",
    ")\n",
    "\n",
    "#Addestramento\n",
    "history1 = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "#Plot\n",
    "pd.DataFrame(history1.history).plot(\n",
    "    figsize=(8, 5), xlim=[0, 40], ylim=[0, 3], grid=True,\n",
    "    xlabel=\"Epoch\", style=[\"r--\", \"r--.\", \"b-\", \"b-*\"]\n",
    ")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie principali\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "# Funzione per generare automaticamente la directory dei log di TensorBoard\n",
    "def get_run_logdir(root_logdir=\"my_logs\"):\n",
    "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# Imposta la directory dei log per TensorBoard\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "# Caricamento del dataset California Housing\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Suddivisione in training, validation e test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Standardizzazione delle feature (molto importante per le reti neurali)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Funzione che costruisce dinamicamente il modello, ricevendo un oggetto hp (HyperParameters)\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=X_train.shape[1:]))\n",
    "\n",
    "    # Costruisce un numero variabile di layer nascosti\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=hp.Int(f\"units_{i}\", min_value=16, max_value=128, step=16),\n",
    "            activation=\"relu\"))\n",
    "\n",
    "    # Layer di output per regressione (1 neurone, senza attivazione)\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    # Ottimizzatore Adam con tasso di apprendimento variabile\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    )\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Callback di TensorBoard per registrare log durante l'addestramento\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=run_logdir, profile_batch=(100, 200))\n",
    "\n",
    "# Tuner per ricerca bayesiana degli iperparametri\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,  # Funzione di costruzione del modello\n",
    "    objective=\"val_root_mean_squared_error\",  # Obiettivo da minimizzare\n",
    "    max_trials=10,  # Numero massimo di combinazioni da testare\n",
    "    directory=\"kt_dir\",  # Directory dove salvare i log dei trial\n",
    "    project_name=\"bayes_housing\",  # Nome del progetto\n",
    "    overwrite=True,  # Sovrascrive risultati precedenti\n",
    ")\n",
    "\n",
    "# Ricerca dei migliori iperparametri su 10 trial\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_cb]  # Integrazione di TensorBoard\n",
    ")\n",
    "\n",
    "# Recupero del miglior modello trovato\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Visualizzazione degli iperparametri migliori\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Migliori iperparametri trovati:\", best_hp.values)\n",
    "\n",
    "# Addestramento finale del miglior modello su tutto il dataset (train + valid)\n",
    "X_full = np.vstack([X_train, X_valid])\n",
    "y_full = np.hstack([y_train, y_valid])\n",
    "best_model.fit(X_full, y_full, epochs=40)\n",
    "\n",
    "# Valutazione finale sul test set\n",
    "test_loss, test_rmse = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test RMSE finale:\", test_rmse)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
